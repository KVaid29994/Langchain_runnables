{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b530c02d",
   "metadata": {},
   "source": [
    "# ðŸ—ï¸ **LangChain Runnables: The Architectural Backbone of LLM Orchestration**  \n",
    "\n",
    "## ðŸš€ **Introduction**  \n",
    "LangChain **Runnables** represent a **paradigm shift** in how developers interact with **large language models (LLMs)** and auxiliary components in AI-driven applications.  \n",
    "\n",
    "Born out of the **chaos** of early LLM integration challenges, Runnables emerged as the **glue** that binds disparate toolsâ€”**models, retrievers, parsers**, and moreâ€”into **cohesive, production-grade workflows**.  \n",
    "\n",
    "Letâ€™s dissect their **origins, purpose, and transformative impact**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b94b844",
   "metadata": {},
   "source": [
    "### ðŸŒ The Pre-Runnable Era: Chaos in LLM Land"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acfefe9",
   "metadata": {},
   "source": [
    "Before Runnables, integrating LLMs into applications was akin to herding cats. Developers faced:\n",
    "\n",
    "- Inconsistent Interfaces: Every component (LLM, parser, retriever) had unique APIs, forcing custom glue code.\n",
    "\n",
    "- Brittle Workflows: Simple tasks like chaining a prompt to a model required manual input/output stitching, leading to fragile pipelines.\n",
    "\n",
    "- Scalability Nightmares: Handling async operations, batch processing, or streaming required reinventing the wheel for each use case.\n",
    "\n",
    "- Limited Reusability: Components couldnâ€™t be easily composed, forcing developers to rebuild similar logic across projects.\n",
    "\n",
    "- This fragmentation stifled innovation. Enter Runnablesâ€”a unified abstraction layer inspired by decades-old software design principles, reimagined for the LLM age.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acb6385",
   "metadata": {},
   "source": [
    "> ### ðŸ§  The \"Aha!\" Moment: Why Runnables Were Needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4c7825",
   "metadata": {},
   "source": [
    "***LangChainâ€™s creators identified three core needs:***\n",
    "\n",
    "- ***Consistency***: A single interface for all components (LLMs, tools, utilities) to reduce cognitive load.\n",
    "\n",
    "- ***Composability***: Modular building blocks that snap together like LEGOÂ® bricks, enabling complex workflows.\n",
    "\n",
    "- ***Scalability***: Native support for modern compute patternsâ€”async, batching, streamingâ€”without boilerplate.\n",
    "\n",
    "\n",
    ">> Runnables solved these by adopting the Command Pattern, a proven behavioral design pattern that encapsulates operations as objects. This allowed:\n",
    "\n",
    "- Dynamic Workflow Assembly: Chains, branches, and parallel tasks could be declared programmatically.\n",
    "\n",
    "- Cross-Component Interop: A retrieverâ€™s output seamlessly flows into a prompt template, then an LLM, then a parser.\n",
    "\n",
    "- Enterprise-Grade Execution: Built-in retries, fallbacks, and observability hooks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9af1cc0",
   "metadata": {},
   "source": [
    "# âš™ï¸ **Under the Hood: How Runnables Work**  \n",
    "\n",
    "## ðŸ”§ Core Mechanics  \n",
    "- **Uniform Interface** ðŸ—ï¸: All Runnables implement `invoke`, `batch`, `stream`, and async variants, ensuring predictable interaction.  \n",
    "- **Schema Enforcement** ðŸ›¡ï¸: Automatic input/output validation using **Pydantic** models, catching errors early.  \n",
    "- **LCEL Integration** ðŸ”—: The **LangChain Expression Language (LCEL)** allows declarative chaining via `|` syntax, e.g., `prompt | model | parser`.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f1f6fd",
   "metadata": {},
   "source": [
    "## ðŸš€ **Key Innovations**  \n",
    "- ðŸŒ€ **RunnableLambda**: Wrap any function into a Runnable, bridging custom logic with LangChainâ€™s ecosystem.  \n",
    "- ðŸ”€ **RunnableBranch**: Implement conditional logic (e.g., routing queries to specialist models).  \n",
    "- âš¡ **RunnableParallel**: Execute multiple Runnables concurrently, merging results.  \n",
    "- ðŸ”„ **RunnableRetry/Fallback**: Automatically retry failed operations or switch to backup components.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ff1bb8",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸŒŸ **Impact: What Runnables Enabled**  \n",
    "### âš¡ Rapid Prototyping  \n",
    "âœ… Developers now assemble proof-of-concepts **in minutes**, not days.  \n",
    "âœ… **Example**: A retrieval-augmented QA system becomes a **5-line LCEL chain**.  \n",
    "\n",
    "### ðŸ“ˆ Scalable Productionization  \n",
    "âœ… **Async support** handles **10x more** concurrent requests.  \n",
    "âœ… **Batch processing** slashes costs for bulk operations (e.g., processing **10K documents**).  \n",
    "\n",
    "### ðŸŽ¯ Observability & Control  \n",
    "âœ… **astream_log** streams intermediate steps for **real-time monitoring**.  \n",
    "âœ… **Schema inspection (`get_input_schema`)** enables **auto-generated API docs**.  \n",
    "\n",
    "### ðŸ”— Ecosystem Growth  \n",
    "âœ… **Third-party integrations** (e.g., arXiv, AWS) adopt Runnable interfaces, expanding **LangChainâ€™s toolkit**.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e05601",
   "metadata": {},
   "source": [
    "### ***âœ… Types of Runnable in LangChain***\n",
    "\n",
    "LangChain introduced the `Runnable` interface to **standardize and simplify** how components (like LLMs, prompts, tools, etc.) can be composed into **chains**. Each `Runnable` represents a single unit of computation that can be invoked using the `.invoke()` method.\n",
    "\n",
    "There are two broad categories:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5f4fb7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ”¹ 1. Runnable Primitives\n",
    "\n",
    "These are the **foundational building blocks** used to compose more complex chains. They are **lightweight wrappers** around basic Python logic or transformations.\n",
    "\n",
    "| **Runnable**        | **Description**                                                                 | **Use Case**                                                   |\n",
    "|---------------------|----------------------------------------------------------------------------------|----------------------------------------------------------------|\n",
    "| `RunnableLambda`    | Wraps a Python function or lambda into a runnable interface                     | Quick transformations, filters, mappings                       |\n",
    "| `RunnableMap`       | Applies a Runnable (or chain) **independently** to each element in a list       | When dealing with lists of inputs (e.g., multiple prompts)     |\n",
    "| `RunnableSequence`  | Chains multiple runnables together sequentially                                 | Define pipelines (Prompt â†’ LLM â†’ Parser)                       |\n",
    "| `RunnableParallel`  | Runs multiple runnables **simultaneously** on the same input                    | Get different outputs at once (e.g., classification + summary) |\n",
    "| `RunnableBranch`    | Conditional routing based on the input       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d290a458",
   "metadata": {},
   "source": [
    "## ðŸ”¹ 2. Task-Specific Runnables\n",
    "\n",
    "These are **prebuilt LangChain components** for specific NLP or AI tasks, such as prompting, model invocation, or document retrieval.\n",
    "\n",
    "| **Runnable**           | **Description**                                                                 |\n",
    "|------------------------|----------------------------------------------------------------------------------|\n",
    "| `PromptTemplate`       | Converts structured input into a natural language prompt                        |\n",
    "| `ChatModel / LLM`      | Invokes a language model using the prompt                                       |\n",
    "| `OutputParser`         | Parses the raw output from the model (e.g., JSON, text, sentiment)              |\n",
    "| `Retriever`            | Retrieves documents from a vector store or search engine                        |\n",
    "| `Tool`                 | Wraps external functions, APIs, or scripts into a LangChain-compatible tool     |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ” Composing Runnables\n",
    "\n",
    "You can chain runnables using the `|` pipe operator:\n",
    "\n",
    "```python\n",
    "chain = prompt_template | llm_model | output_parser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1db2419",
   "metadata": {},
   "source": [
    "## ðŸ§  LangChain: Runnable Sequence Guide\n",
    "\n",
    "LangChain's `RunnableSequence` is a powerful tool for chaining together multiple components (like prompts, models, and output parsers) into a single, reusable sequence.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## ðŸ§© What is `RunnableSequence`?\n",
    "\n",
    "A `RunnableSequence` is a **pipeline** that allows you to execute multiple steps in order â€” like function composition but designed specifically for LangChain primitives.\n",
    "\n",
    "It follows the **Functional Programming** style:  \n",
    "Each step takes input, processes it, and passes it to the next.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ” Key Characteristics\n",
    "\n",
    "- Composable ðŸ› ï¸  \n",
    "- Type-safe âœ”ï¸  \n",
    "- Reusable ðŸ”„  \n",
    "- Asynchronous-friendly â±ï¸  \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ—ï¸ How to Create a RunnableSequence\n",
    "\n",
    "You typically create a `RunnableSequence` using the `|` (pipe) operator to chain components together.\n",
    "\n",
    "### âœ… Example:\n",
    "\n",
    "```python\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.output_parsers import StrOutputParser\n",
    "\n",
    "# Step 1: Create a prompt\n",
    "prompt = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "\n",
    "# Step 2: Choose your LLM\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "# Step 3: Output parser\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Step 4: Chain them into a sequence\n",
    "chain: RunnableSequence = prompt | llm | parser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5146331",
   "metadata": {},
   "source": [
    "# ðŸ§  LangChain: RunnableParallel Guide\n",
    "\n",
    "`RunnableParallel` is used when you want to **run multiple chains (or functions) in parallel**, each getting the **same input** and returning a **dictionary of outputs**.\n",
    "\n",
    "---\n",
    "\n",
    "## âš¡ What is `RunnableParallel`?\n",
    "\n",
    "Itâ€™s a LangChain utility that takes a dictionary of runnables and executes them in parallel.  \n",
    "Great for **multi-tasking**, **parallel LLM calls**, or generating **multiple outputs** from the same input.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš¦ Key Features\n",
    "\n",
    "- Run multiple pipelines in parallel ðŸ›¤ï¸  \n",
    "- Accepts same input for all branches ðŸ“¥  \n",
    "- Returns a dict of outputs ðŸ“¤  \n",
    "- Supports sync and async ðŸ”  \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ—ï¸ Structure\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "parallel_chain = RunnableParallel({\n",
    "    \"joke\": chain_1,\n",
    "    \"poem\": chain_2,\n",
    "    \"summary\": chain_3\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57e0aa7",
   "metadata": {},
   "source": [
    "### âœ… Example: Generate Joke & Poem\n",
    "```python\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "llm = OpenAI(temperature=0.7)\n",
    "\n",
    "# Prompt 1: Joke\n",
    "joke_prompt = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "joke_chain = joke_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Prompt 2: Poem\n",
    "poem_prompt = PromptTemplate.from_template(\"Write a short poem about {topic}\")\n",
    "poem_chain = poem_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Combine into parallel\n",
    "parallel_chain = RunnableParallel({\n",
    "    \"joke\": joke_chain,\n",
    "    \"poem\": poem_chain\n",
    "})\n",
    "\n",
    "# Run it\n",
    "result = parallel_chain.invoke({\"topic\": \"penguins\"})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bd17e7",
   "metadata": {},
   "source": [
    "## ***ðŸ§  LangChain: RunnableLambda in Python***\n",
    "The RunnableLambda is part of the LangChain Expression Language and provides a way to wrap any Python function into a chainable, composable LangChain runnable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b351bfba",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b16be8e",
   "metadata": {},
   "source": [
    "### âœ¨ What is RunnableLambda?\n",
    "- RunnableLambda lets you wrap arbitrary Python functions and make them compatible with the LangChain Runnable ecosystem.\n",
    "- This is useful when you want to insert custom logic in a LangChain pipeline, such as modifying inputs, logging, filtering, or any transformation.\n",
    "- from langchain_core.runnables import RunnableLambda\n",
    "``` python\n",
    "\n",
    "# A simple lambda function\n",
    "def to_uppercase(text: str) -> str:\n",
    "    return text.upper()\n",
    "\n",
    "# Wrap it into a RunnableLambda\n",
    "uppercase_runnable = RunnableLambda(to_uppercase)\n",
    "\n",
    "# Invoke it\n",
    "result = uppercase_runnable.invoke(\"hello langchain!\")\n",
    "print(result)  # Output: HELLO LANGCHAIN!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe7b66a",
   "metadata": {},
   "source": [
    "# ðŸŒ¿ LangChain: RunnableBranch Guide\n",
    "\n",
    "`RunnableBranch` is used to **conditionally route input** to different chains or functions based on custom logic â€” like an `if-else` structure for LangChain pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ¤” What is `RunnableBranch`?\n",
    "\n",
    "Think of `RunnableBranch` as a **switch-case** or **decision router**.  \n",
    "It evaluates input using a series of conditions and sends the input to the **first matching runnable**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§© Structure\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables import RunnableBranch\n",
    "\n",
    "branch_chain = RunnableBranch(\n",
    "    (condition_1, runnable_1),\n",
    "    (condition_2, runnable_2),\n",
    "    ...,\n",
    "    default_runnable\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c954c34",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
