{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b530c02d",
   "metadata": {},
   "source": [
    "# 🏗️ **LangChain Runnables: The Architectural Backbone of LLM Orchestration**  \n",
    "\n",
    "## 🚀 **Introduction**  \n",
    "LangChain **Runnables** represent a **paradigm shift** in how developers interact with **large language models (LLMs)** and auxiliary components in AI-driven applications.  \n",
    "\n",
    "Born out of the **chaos** of early LLM integration challenges, Runnables emerged as the **glue** that binds disparate tools—**models, retrievers, parsers**, and more—into **cohesive, production-grade workflows**.  \n",
    "\n",
    "Let’s dissect their **origins, purpose, and transformative impact**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b94b844",
   "metadata": {},
   "source": [
    "### 🌍 The Pre-Runnable Era: Chaos in LLM Land"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acfefe9",
   "metadata": {},
   "source": [
    "Before Runnables, integrating LLMs into applications was akin to herding cats. Developers faced:\n",
    "\n",
    "- Inconsistent Interfaces: Every component (LLM, parser, retriever) had unique APIs, forcing custom glue code.\n",
    "\n",
    "- Brittle Workflows: Simple tasks like chaining a prompt to a model required manual input/output stitching, leading to fragile pipelines.\n",
    "\n",
    "- Scalability Nightmares: Handling async operations, batch processing, or streaming required reinventing the wheel for each use case.\n",
    "\n",
    "- Limited Reusability: Components couldn’t be easily composed, forcing developers to rebuild similar logic across projects.\n",
    "\n",
    "- This fragmentation stifled innovation. Enter Runnables—a unified abstraction layer inspired by decades-old software design principles, reimagined for the LLM age.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acb6385",
   "metadata": {},
   "source": [
    "> ### 🧠 The \"Aha!\" Moment: Why Runnables Were Needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4c7825",
   "metadata": {},
   "source": [
    "***LangChain’s creators identified three core needs:***\n",
    "\n",
    "- ***Consistency***: A single interface for all components (LLMs, tools, utilities) to reduce cognitive load.\n",
    "\n",
    "- ***Composability***: Modular building blocks that snap together like LEGO® bricks, enabling complex workflows.\n",
    "\n",
    "- ***Scalability***: Native support for modern compute patterns—async, batching, streaming—without boilerplate.\n",
    "\n",
    "\n",
    ">> Runnables solved these by adopting the Command Pattern, a proven behavioral design pattern that encapsulates operations as objects. This allowed:\n",
    "\n",
    "- Dynamic Workflow Assembly: Chains, branches, and parallel tasks could be declared programmatically.\n",
    "\n",
    "- Cross-Component Interop: A retriever’s output seamlessly flows into a prompt template, then an LLM, then a parser.\n",
    "\n",
    "- Enterprise-Grade Execution: Built-in retries, fallbacks, and observability hooks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9af1cc0",
   "metadata": {},
   "source": [
    "# ⚙️ **Under the Hood: How Runnables Work**  \n",
    "\n",
    "## 🔧 Core Mechanics  \n",
    "- **Uniform Interface** 🏗️: All Runnables implement `invoke`, `batch`, `stream`, and async variants, ensuring predictable interaction.  \n",
    "- **Schema Enforcement** 🛡️: Automatic input/output validation using **Pydantic** models, catching errors early.  \n",
    "- **LCEL Integration** 🔗: The **LangChain Expression Language (LCEL)** allows declarative chaining via `|` syntax, e.g., `prompt | model | parser`.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f1f6fd",
   "metadata": {},
   "source": [
    "## 🚀 **Key Innovations**  \n",
    "- 🌀 **RunnableLambda**: Wrap any function into a Runnable, bridging custom logic with LangChain’s ecosystem.  \n",
    "- 🔀 **RunnableBranch**: Implement conditional logic (e.g., routing queries to specialist models).  \n",
    "- ⚡ **RunnableParallel**: Execute multiple Runnables concurrently, merging results.  \n",
    "- 🔄 **RunnableRetry/Fallback**: Automatically retry failed operations or switch to backup components.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ff1bb8",
   "metadata": {},
   "source": [
    "\n",
    "## 🌟 **Impact: What Runnables Enabled**  \n",
    "### ⚡ Rapid Prototyping  \n",
    "✅ Developers now assemble proof-of-concepts **in minutes**, not days.  \n",
    "✅ **Example**: A retrieval-augmented QA system becomes a **5-line LCEL chain**.  \n",
    "\n",
    "### 📈 Scalable Productionization  \n",
    "✅ **Async support** handles **10x more** concurrent requests.  \n",
    "✅ **Batch processing** slashes costs for bulk operations (e.g., processing **10K documents**).  \n",
    "\n",
    "### 🎯 Observability & Control  \n",
    "✅ **astream_log** streams intermediate steps for **real-time monitoring**.  \n",
    "✅ **Schema inspection (`get_input_schema`)** enables **auto-generated API docs**.  \n",
    "\n",
    "### 🔗 Ecosystem Growth  \n",
    "✅ **Third-party integrations** (e.g., arXiv, AWS) adopt Runnable interfaces, expanding **LangChain’s toolkit**.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e05601",
   "metadata": {},
   "source": [
    "### ***✅ Types of Runnable in LangChain***\n",
    "\n",
    "LangChain introduced the `Runnable` interface to **standardize and simplify** how components (like LLMs, prompts, tools, etc.) can be composed into **chains**. Each `Runnable` represents a single unit of computation that can be invoked using the `.invoke()` method.\n",
    "\n",
    "There are two broad categories:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5f4fb7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🔹 1. Runnable Primitives\n",
    "\n",
    "These are the **foundational building blocks** used to compose more complex chains. They are **lightweight wrappers** around basic Python logic or transformations.\n",
    "\n",
    "| **Runnable**        | **Description**                                                                 | **Use Case**                                                   |\n",
    "|---------------------|----------------------------------------------------------------------------------|----------------------------------------------------------------|\n",
    "| `RunnableLambda`    | Wraps a Python function or lambda into a runnable interface                     | Quick transformations, filters, mappings                       |\n",
    "| `RunnableMap`       | Applies a Runnable (or chain) **independently** to each element in a list       | When dealing with lists of inputs (e.g., multiple prompts)     |\n",
    "| `RunnableSequence`  | Chains multiple runnables together sequentially                                 | Define pipelines (Prompt → LLM → Parser)                       |\n",
    "| `RunnableParallel`  | Runs multiple runnables **simultaneously** on the same input                    | Get different outputs at once (e.g., classification + summary) |\n",
    "| `RunnableBranch`    | Conditional routing based on the input       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d290a458",
   "metadata": {},
   "source": [
    "## 🔹 2. Task-Specific Runnables\n",
    "\n",
    "These are **prebuilt LangChain components** for specific NLP or AI tasks, such as prompting, model invocation, or document retrieval.\n",
    "\n",
    "| **Runnable**           | **Description**                                                                 |\n",
    "|------------------------|----------------------------------------------------------------------------------|\n",
    "| `PromptTemplate`       | Converts structured input into a natural language prompt                        |\n",
    "| `ChatModel / LLM`      | Invokes a language model using the prompt                                       |\n",
    "| `OutputParser`         | Parses the raw output from the model (e.g., JSON, text, sentiment)              |\n",
    "| `Retriever`            | Retrieves documents from a vector store or search engine                        |\n",
    "| `Tool`                 | Wraps external functions, APIs, or scripts into a LangChain-compatible tool     |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 Composing Runnables\n",
    "\n",
    "You can chain runnables using the `|` pipe operator:\n",
    "\n",
    "```python\n",
    "chain = prompt_template | llm_model | output_parser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1db2419",
   "metadata": {},
   "source": [
    "## 🧠 LangChain: Runnable Sequence Guide\n",
    "\n",
    "LangChain's `RunnableSequence` is a powerful tool for chaining together multiple components (like prompts, models, and output parsers) into a single, reusable sequence.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 🧩 What is `RunnableSequence`?\n",
    "\n",
    "A `RunnableSequence` is a **pipeline** that allows you to execute multiple steps in order — like function composition but designed specifically for LangChain primitives.\n",
    "\n",
    "It follows the **Functional Programming** style:  \n",
    "Each step takes input, processes it, and passes it to the next.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 Key Characteristics\n",
    "\n",
    "- Composable 🛠️  \n",
    "- Type-safe ✔️  \n",
    "- Reusable 🔄  \n",
    "- Asynchronous-friendly ⏱️  \n",
    "\n",
    "---\n",
    "\n",
    "## 🏗️ How to Create a RunnableSequence\n",
    "\n",
    "You typically create a `RunnableSequence` using the `|` (pipe) operator to chain components together.\n",
    "\n",
    "### ✅ Example:\n",
    "\n",
    "```python\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.output_parsers import StrOutputParser\n",
    "\n",
    "# Step 1: Create a prompt\n",
    "prompt = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "\n",
    "# Step 2: Choose your LLM\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "# Step 3: Output parser\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Step 4: Chain them into a sequence\n",
    "chain: RunnableSequence = prompt | llm | parser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5146331",
   "metadata": {},
   "source": [
    "# 🧠 LangChain: RunnableParallel Guide\n",
    "\n",
    "`RunnableParallel` is used when you want to **run multiple chains (or functions) in parallel**, each getting the **same input** and returning a **dictionary of outputs**.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚡ What is `RunnableParallel`?\n",
    "\n",
    "It’s a LangChain utility that takes a dictionary of runnables and executes them in parallel.  \n",
    "Great for **multi-tasking**, **parallel LLM calls**, or generating **multiple outputs** from the same input.\n",
    "\n",
    "---\n",
    "\n",
    "## 🚦 Key Features\n",
    "\n",
    "- Run multiple pipelines in parallel 🛤️  \n",
    "- Accepts same input for all branches 📥  \n",
    "- Returns a dict of outputs 📤  \n",
    "- Supports sync and async 🔁  \n",
    "\n",
    "---\n",
    "\n",
    "## 🏗️ Structure\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "parallel_chain = RunnableParallel({\n",
    "    \"joke\": chain_1,\n",
    "    \"poem\": chain_2,\n",
    "    \"summary\": chain_3\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57e0aa7",
   "metadata": {},
   "source": [
    "### ✅ Example: Generate Joke & Poem\n",
    "```python\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "llm = OpenAI(temperature=0.7)\n",
    "\n",
    "# Prompt 1: Joke\n",
    "joke_prompt = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "joke_chain = joke_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Prompt 2: Poem\n",
    "poem_prompt = PromptTemplate.from_template(\"Write a short poem about {topic}\")\n",
    "poem_chain = poem_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Combine into parallel\n",
    "parallel_chain = RunnableParallel({\n",
    "    \"joke\": joke_chain,\n",
    "    \"poem\": poem_chain\n",
    "})\n",
    "\n",
    "# Run it\n",
    "result = parallel_chain.invoke({\"topic\": \"penguins\"})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bd17e7",
   "metadata": {},
   "source": [
    "## ***🧠 LangChain: RunnableLambda in Python***\n",
    "The RunnableLambda is part of the LangChain Expression Language and provides a way to wrap any Python function into a chainable, composable LangChain runnable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b351bfba",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b16be8e",
   "metadata": {},
   "source": [
    "### ✨ What is RunnableLambda?\n",
    "- RunnableLambda lets you wrap arbitrary Python functions and make them compatible with the LangChain Runnable ecosystem.\n",
    "- This is useful when you want to insert custom logic in a LangChain pipeline, such as modifying inputs, logging, filtering, or any transformation.\n",
    "- from langchain_core.runnables import RunnableLambda\n",
    "``` python\n",
    "\n",
    "# A simple lambda function\n",
    "def to_uppercase(text: str) -> str:\n",
    "    return text.upper()\n",
    "\n",
    "# Wrap it into a RunnableLambda\n",
    "uppercase_runnable = RunnableLambda(to_uppercase)\n",
    "\n",
    "# Invoke it\n",
    "result = uppercase_runnable.invoke(\"hello langchain!\")\n",
    "print(result)  # Output: HELLO LANGCHAIN!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe7b66a",
   "metadata": {},
   "source": [
    "# 🌿 LangChain: RunnableBranch Guide\n",
    "\n",
    "`RunnableBranch` is used to **conditionally route input** to different chains or functions based on custom logic — like an `if-else` structure for LangChain pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "## 🤔 What is `RunnableBranch`?\n",
    "\n",
    "Think of `RunnableBranch` as a **switch-case** or **decision router**.  \n",
    "It evaluates input using a series of conditions and sends the input to the **first matching runnable**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 Structure\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables import RunnableBranch\n",
    "\n",
    "branch_chain = RunnableBranch(\n",
    "    (condition_1, runnable_1),\n",
    "    (condition_2, runnable_2),\n",
    "    ...,\n",
    "    default_runnable\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c954c34",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
